"""
Used to load simulation results from pywrdrb output files.

Overview:
This module defines the Output class which provides functionality for loading
simulation output data from HDF5 files generated by pywrdrb model runs.
This class is built on top of the AbstractDataLoader class such that it has the same
data structure as the other data loaders (e.g. Observations) and is used in the 
general pywrdrb.Data() class for Data().load_output().

Technical Notes:
- Handles loading multiple output files with multiple scenarios
- Requires specification of results_set options 
- #TODO:
    - Add {output_file : label} mapping, such that the stored output label can be changed instead of the file name.

Links:
- See results_set options in the docs: https://pywr-drb.github.io/Pywr-DRB/results_set_options.html

Change Log:
TJA, 2025-05-02, Added consistent docstrings.
"""

import numpy as np
import pandas as pd
import h5py

from pywrdrb.utils.constants import mg_to_mcm
from pywrdrb.utils.results_sets import pywrdrb_results_set_opts
from pywrdrb.utils.lists import (
    reservoir_list,
    reservoir_list_nyc,
    majorflow_list,
    reservoir_link_pairs,
    drbc_lower_basin_reservoirs
)

from pywrdrb.load.abstract_loader import AbstractDataLoader, default_kwargs
from pywrdrb.utils.hdf5 import get_n_scenarios_from_pywrdrb_output_file


class Output(AbstractDataLoader):
    """
    Loader for simulation output data from pywrdrb.
    
    Loads data from HDF5 output files and organizes it by results_set and scenario.
    
    Methods
    -------
    load(output_filenames, **kwargs)
        Load data for the specified output files and results_sets.
    
    Attributes
    ----------
    pn : PathNavigator
        Path navigator object for directory management. Default is the global pn.
    output_filenames_with_filetype : list
        List of output files with their file extensions.
    output_labels_and_files : dict
        Dictionary mapping output labels to filenames.
    output_labels : list
        List of output labels extracted from filenames.
    """
    def __init__(self, 
                 output_filenames, 
                 **kwargs):
        """
        Initialize the Output loader with filenames and other options.

        Parameters
        ----------
        output_filenames : list
            List of output files to load. Files can include or exclude file extensions.
        results_sets : list, optional
            Results sets to load.
        units : str, optional
            Units for the results (default 'MG').
        pn : PathNavigator, optional
            PathNavigator object with defined directories. Default is global pn.
        print_status : bool, optional
            Whether to print status updates (default False).
        """
        # Save output filenames with and without filetype
        self.output_filenames_with_filetype = []
        self.output_labels_and_files = {}
        for f in output_filenames:
            if "." in f:
                self.output_filenames_with_filetype.append(f)
            else:
                self.output_filenames_with_filetype.append(f"{f}.hdf5")

            self.output_labels_and_files[self.__get_output_label_from_filename__(f)] = f
        self.output_labels = list(self.output_labels_and_files.keys())
        
        self.valid_results_set_opts = pywrdrb_results_set_opts
        
        self.default_kwargs = default_kwargs
        super().__parse_kwargs__(self.default_kwargs, 
                              **kwargs)

    def __get_output_label_from_filename__(self, filename):
        """
        Extract a label from a full filename.
        
        The label is the stripped down filename 
        without the path or file extension. This label is used
        in the hierarchical data structure to store the results.
        
        Parameters
        ----------
        filename : str
            Full path of the filename.

        Returns
        -------
        str
        
        Notes
        -----
        (TJA) This should probably be more robust.
        """
        if "/" in filename:
            filename = filename.split("/")[-1]
        elif "\\" in filename:
            filename = filename.split("\\")[-1]
        
        if "." in filename:
            filename = filename.split(".")[0]
        
        return filename
    
    def __get_scenario_ids_for_output_filenames__(self):
        """
        Get the scenario indices for each output file.
        
        Stores a dictionary of scenario indices for each output file,
        where the keys are the output labels and the values are numpy arrays
        of scenario indices. The scenario indices are obtained from the
        pywrdrb output files using the `get_n_scenarios_from_pywrdrb_output_file` function.
        
        Returns
        -------
        None
        """
        self.scenarios = {
            label: np.arange(get_n_scenarios_from_pywrdrb_output_file(file))
            for label, file in self.output_labels_and_files.items()
        }
        
    def get_results_set_opts(self):
        """
        Return a list of valid results_set options for pywrdrb output data.
        
        The results_set strings are used to specify certain categories of 
        simulation variables to extract from the pywrdrb output file,
        when used with pywrdrb.load.get_pywrdrb_results().
        
        For example, "res_storage" will result in a DataFrame containing
        reservoir storage timeseries for all model reservoirs. 
        
        Returns
        -------
        list
            A list of valid results_set options for pywrdrb output data. 
        """
        return pywrdrb_results_set_opts

    def get_keys_and_column_names_for_results_set(self, keys, results_set):
        """
        For given results_set, identify hdf5 key subset and new variable names.
        
        The pywrdrb output file contains a large number of variables which each
        have a different key in the HDF5 file. When loading results, we want to 
        extract a subset of these variables corresponding to a unique results_set.
        E.g., for results_set = "res_storage" we want to keep only keys for the 
        stroage variables. 
        
        Also, we want to rename the keys to be user-friendly, often we rename the 
        variable as simply the node name. These col_names are used to rename columns
        for the loaded pd.DataFrame.
        
        Parameters
        ----------
        keys : list[str]
            The full list of HDF5 keys stored in the pywrdrb output file.
        results_set : str
            The type of results to retrieve. Call pywrdrb.load.get_results_set_opts()
            to see the valid options.
            
        Returns
        -------
        tuple
            (keys, col_names) where keys is a subset of all output hdf5 keys to extract 
            for the given results_set and col_names are the corresponding column names 
            for the final output DataFrame, used to rename the keys.
        """
        if results_set == "all":
            keys = keys
            col_names = [k for k in keys]
            
        elif results_set == "reservoir_downstream_gage":
            ## Need to pull flow data for link_ downstream of reservoirs instead of simulated outflows
            keys_with_link = [
                k
                for k in keys
                if k.split("_")[0] == "link"
                and k.split("_")[1] in reservoir_link_pairs.values()
            ]
            col_names = []
            for k in keys_with_link:
                col_names.append(
                    [
                        res
                        for res, link in reservoir_link_pairs.items()
                        if link == k.split("_")[1]
                    ][0]
                )

            # Now pull simulated relases from un-observed reservoirs
            keys_without_link = [
                k
                for k in keys
                if k.split("_")[0] == "outflow"
                and k.split("_")[1] in reservoir_list
                and k.split("_")[1] not in reservoir_link_pairs.keys()
            ]
            for k in keys_without_link:
                col_names.append(k.split("_")[1])
            keys = keys_with_link + keys_without_link

        elif results_set == "res_storage":
            keys = [
                k
                for k in keys
                if k.split("_")[0] == "reservoir" and k.split("_")[1] in reservoir_list
            ]
            col_names = [k.split("_")[1] for k in keys]
            
            # print(f"Reservoir storage keys: {keys}")
            # print(f"Reservoir storage column names: {col_names}")
            
        elif results_set == "major_flow":
            keys = [
                k
                for k in keys
                if k.split("_")[0] == "link" and k.split("_")[1] in majorflow_list
            ]
            col_names = [k.split("_")[1] for k in keys]
        elif results_set == "res_release":
            ### reservoir releases are "outflow" plus "spill".
            # These are summed later in this function.
            # Not all reservoirs have spill.
            keys_outflow = [f"outflow_{r}" for r in reservoir_list]
            keys_spill = [f"spill_{r}" for r in reservoir_list]
            keys = keys_outflow + keys_spill
            col_names = keys

        elif results_set == "downstream_release_target":
            keys = [f"{results_set}_{reservoir}" for reservoir in reservoir_list_nyc]
            col_names = reservoir_list_nyc

        elif results_set == "inflow":
            keys = [k for k in keys if k.split("_")[0] == "catchment"]
            col_names = [k.split("_")[1] for k in keys]
        elif results_set == "catchment_withdrawal":
            keys = [k for k in keys if k.split("_")[0] == "catchmentWithdrawal"]
            col_names = [k.split("_")[1] for k in keys]
        elif results_set == "catchment_consumption":
            keys = [k for k in keys if k.split("_")[0] == "catchmentConsumption"]
            col_names = [k.split("_")[1] for k in keys]

        elif results_set in (
            "prev_flow_catchmentWithdrawal",
            "max_flow_catchmentWithdrawal",
            "max_flow_catchmentConsumption",
        ):
            keys = [k for k in keys if results_set in k]
            col_names = [k.split("_")[-1] for k in keys]

        elif results_set in ("res_level"):
            keys = [k for k in keys if "drought_level" in k]
            col_names = [k.split("_")[-1] for k in keys]

        elif results_set == "ffmp_level_boundaries":
            keys = [f"level{l}" for l in ["1b", "1c", "2", "3", "4", "5"]]
            col_names = [k for k in keys]
        elif results_set == "mrf_target":
            keys = [k for k in keys if results_set in k]
            col_names = [k.split("mrf_target_")[1] for k in keys]

        elif results_set == "nyc_release_components":
            keys = (
                [
                    f"mrf_target_individual_{reservoir}"
                    for reservoir in reservoir_list_nyc
                ]
                + [f"flood_release_{reservoir}" for reservoir in reservoir_list_nyc]
                + [
                    f"mrf_montagueTrenton_{reservoir}"
                    for reservoir in reservoir_list_nyc
                ]
                + [f"spill_{reservoir}" for reservoir in reservoir_list_nyc]
            )
            col_names = [k for k in keys]
        elif results_set == "lower_basin_mrf_contributions":
            keys = [
                f"mrf_trenton_{reservoir}" for reservoir in drbc_lower_basin_reservoirs
            ]
            col_names = [k for k in keys]
        elif results_set == "ibt_demands":
            keys = ["demand_nyc", "demand_nj"]
            col_names = [k for k in keys]
        elif results_set == "ibt_diversions":
            keys = ["delivery_nyc", "delivery_nj"]
            col_names = [k for k in keys]
        elif results_set == "mrf_targets":
            keys = ["mrf_target_delMontague", "mrf_target_delTrenton"]
            col_names = [k for k in keys]
        elif results_set == "all_mrf":
            keys = [k for k in keys if "mrf" in k]
            col_names = [k for k in keys]
        elif results_set == "temperature":
            keys = ["thermal_release_requirement", "temperature_after_thermal_release_mu", "temperature_after_thermal_release_sd"] \
                + ['forecasted_temperature_before_thermal_release_mu', 'forecasted_temperature_before_thermal_release_sd'] \
                #[k for k in keys if "temperature" in k or "thermal" in k] \
                #+ ['estimated_Q_i', 'estimated_Q_C']
            col_names = [k for k in keys]
        elif results_set == "salinity":
            keys = ["salt_front_location_mu", "salt_front_location_sd"] 
            col_names = [k for k in keys]
        # resulst_set may be a specific key in the model
        elif results_set in keys:
            keys = [results_set]
            col_names = [results_set]
        else:
            #TODO: raise value error
            pass
        return keys, col_names

    def get_pywrdrb_results(self,
        output_filename, 
        results_set="all", 
        scenarios=[0], 
        datetime_index=None, 
        units=None,
        ):
        """
        Extract simulation results from pywrdrb model output file.
        
        Retrieves specified results for specific scenarios from an HDF5 output file
        and organizes them into a dictionary of pandas DataFrames.
        
        Parameters
        ----------
        output_filename : str
            The full output filename from pywrdrb simulation (e.g., "<path>/drb_output_nhmv10.hdf5").
        results_set : str, optional
            The results set to return. Call pywrdrb.load.get_results_set_opts()
            to get a list of the valid options. When results_set='all', then 
            the raw variable name is preserved. Else, the variables are relabled based on their 
            corresponding node names. Default: "all" 
        scenarios : list[int], optional
            The scenario index numbers. Only needed for ensemble simulation results. Default: [0]
        datetime_index : pd.DatetimeIndex, optional
            Existing datetime index to reuse. Creating dates is slow, so reusing is efficient.
        units : str, optional
            Units to convert flow data to. Options: "MG", "MCM"

        Returns
        -------
        tuple
            (dict, pd.DatetimeIndex) where dict maps scenario indices to DataFrames
            of results, and pd.DatetimeIndex is the datetime index used.
        """
        # Validate output file
        output_filename = output_filename if ".hdf5" in output_filename else f"{output_filename}.hdf5"

        # Validate results_set
        if results_set not in pywrdrb_results_set_opts:
            err_msg = f"Invalid results_set specified for get_pywrdrb_results().\n"
            err_msg += f" Valid results_set options: {pywrdrb_results_set_opts}"
            raise ValueError(err_msg)

        # Get result data from HDF5 output file
        with h5py.File(output_filename, "r") as f:
            all_keys = list(f.keys())
            keys, col_names = self.get_keys_and_column_names_for_results_set(keys=all_keys, 
                                                                        results_set=results_set)

            data = []
            # Now pull the data using keys
            for k in keys:
                data.append(f[k][:, scenarios])

            # Convert data to 3D array
            data = np.stack(data, axis=2)

            if units is not None:
                if units == "MG":
                    pass
                elif units == "MCM":
                    data *= mg_to_mcm

            if datetime_index is not None:
                if len(datetime_index) == len(f["time"]):
                    reuse_datetime_index = True
                else:
                    reuse_datetime_index = False
            else:
                reuse_datetime_index = False

            if not reuse_datetime_index:
                # Format datetime index
                time = f["time"][:]
                
                ### custom OutputRecorder requires this:
                if type(time[0]) == bytes:
                    datetime_index = pd.to_datetime([t.decode('utf-8') for t in f['time'][:]])

                ### TablesRecorder requires this:
                else:                
                    day = [f["time"][i][0] for i in range(len(f["time"]))]
                    month = [f["time"][i][2] for i in range(len(f["time"]))]
                    year = [f["time"][i][3] for i in range(len(f["time"]))]
                    date = [f"{y}-{m}-{d}" for y, m, d in zip(year, month, day)]
                    datetime_index = pd.to_datetime(date)


            # Now store each scenario as individual pd.DataFrames in the dict
            results_dict = {}
            for s in scenarios:
                results_dict[s] = pd.DataFrame(
                    data[:, s, :], columns=col_names, index=datetime_index
                )

            # If results_set is 'res_release', sum the outflow and spill data,
            # columns with the same reservoir name
            if results_set == "res_release":
                for s in scenarios:
                    for r in reservoir_list:
                        release_cols = [c for c in results_dict[s].columns if r in c]

                        # sum
                        if len(release_cols) > 1:
                            results_dict[s][r] = results_dict[s][release_cols].sum(axis=1)
                            results_dict[s] = results_dict[s].drop(release_cols, axis=1)
                        else:
                            results_dict[s] = results_dict[s].rename(
                                columns={release_cols[0]: r}
                            )
            
            # For temp and salinity LSTM outputs, model output is lag 1;
            # we need to shift the data 1 day
            if results_set in ["temperature"]:
                for s in scenarios:
                    # shift all columns except "thermal_release_requirement" for temp
                    for col in results_dict[s].columns:
                        if col != "thermal_release_requirement":
                            results_dict[s][col] = results_dict[s][col].shift(-1)
                            
            if results_set in ["salinity"]:
                for s in scenarios:
                    results_dict[s] = results_dict[s].shift(-1)

                
            return results_dict, datetime_index

    def load(self, **kwargs):
        """
        Load output data based on filenames and results sets.
        
        Loads the data from the output files for the specified results sets and
        stores them as attributes of the Output object.

        Parameters
        ----------
        results_sets : list, optional
            Results sets to load.
        print_status : bool, optional
            Whether to print status updates.

        Returns
        -------
        None
        """

        super().__parse_kwargs__(default_kwargs=self.default_kwargs,
                              **kwargs)

        super().__validate_results_sets__(valid_results_set_opts=self.valid_results_set_opts)

        super().__verify_files_exist__(files=self.output_filenames_with_filetype)

        self.__get_scenario_ids_for_output_filenames__()

        # Load the results
        all_results_data = {}

        datetime = None
        for s in self.results_sets:
            all_results_data[s] = {}
            
            for label, file in self.output_labels_and_files.items():
                if self.print_status:
                    print(f"Loading {s} data from {label}")


                all_results_data[s][label], datetime = self.get_pywrdrb_results(
                    output_filename=file,
                    results_set=s,
                    scenarios=self.scenarios[label],
                    datetime_index=datetime,
                    units=self.units,
                )
            
            self.set_data(data = all_results_data[s],
                             name = s)
